#define inputSize  10
#define hiddenSize  3
#define outputSize 5

struct NeuralNet
{
  double alpha;

  int inputLayer[inputSize];

  double hiddenWeight[hiddenSize][inputSize + 1];
  double hiddenLayer[hiddenSize];

  double outputWeight[outputSize][hiddenSize + 1];
  double outputLayer[outputSize];
};

struct RLParams
{
  double alpha;
  double gamma;
  double epsilon;
  double seed;
  int threshold;
};

void onRL(float speed_l, float speed_r)
{
  OnFwd(OUT_B, speed_r);
  OnFwd(OUT_C, speed_l);
};

double sigmoid(double x)
{
  double s;
  s = 1 / (1 - exp(-1*x));
  return s;
}

void initHiddenWeight()
{
  int i, j;
  for(i = 0; i < hiddenSize; i++)
  {
    for(j = 0; j < inputSize; j++)
    {
      NN.hiddenWeight[i][j] = rand();
    }
  }
}

void initOutputWeight()
{
  int i, j;
  for (i = 0; i < outputSize; i++) {
    for (j = 0; i < hiddenSize; j++) {
      NN.outputWeight[i][j] = rand();
    }
  }
}

void initNN()
{
  initHiddenWeight();
  initOutputWeight();
}


void forwardProp()
{
  int i, j;

  for (i = 0; i < hiddenSize; i++) 
  {
    NN.hiddenLayer[i] = 0;
    for (j = 0; j < inputSize; j++) 
    {
      NN.hiddenLayer[i] += NN.inputLayer[j] * NN.hiddenWeight[i][j];
      NN.hiddenLayer[i] -= NN.hiddenWeight[i][j];
    }
    NN.hiddenLayer[i] = sigmoid(NN.hiddenLayer[i]);
  }

  for (i = 0; i < outputSize; i++)
  {
    NN.outputLayer[i] = 0;
    for (j = 0; j < hiddenSize; j++)
    {
      NN.outputLayer[i] += NN.hiddenLayer[i] * NN.outputWeight[i][j];
      NN.outputLayer[i] -= NN.outputWeight[i][j];
    }
    NN.outputLayer[i] = sigmoid(NN.outputLayer[i]);
  }
}

void backProp(yHat, yTrue)
{
  int i, j;
  double diffW;
  double error;

  error = yHat - yTrue;

  // update hiddenWeight
  for (i = 0; i < hiddenSize; i++)
  {
    for (j = 0; j < inputSize; j++)
    {
      diffW = NN.hiddenLayer[i] * (1 - NN.hiddenLayer[i]) * NN.hiddenWeight[i][j] * error * yTrue * (1 - yTrue);
      NN.hiddenWeight[i][j] = NN.hiddenWeight[i][j] + NN.alpha * NN.inputLayer[j] * diffW;
    }
  }

  // update outputWeight
  for (i = 0; i < hiddenSize; i++)
  {
    for (j = 0; j < inputSize; j++)
    {
      diffW = NN.outputWeight[i][j] * error * y * (1 - y);
      NN.outputWeight[i][j] = NN.outputWeight[i][j] + NN.alpha * NN.inputLayer[j] * diffW;
    }
  }
}

///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////
// Reinforcement Learning Part
///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////



double getQValue(int state)
{
  NN.inputLayer[state] = 1;
  forwardProp(); // update NN.outputLayer
  return argmax(NN.outputLayer);
}

int selectAction(int state, int num)
{
  int action;
  double Qvalues;

  action = getQValue(state);
  return action;
};

int epsilonGreedy(int eps, int state, int num)
{
  int action;
  if (eps > rand() % 100)
  {
    action = rand() % num;
  }
  else
  {
    action = selectAction(state, num);
  }
  return action;
};

void updateWeight(int state, int nextState, double reward)
{
  double currentQvalue;
  double labeledQvalue;

  currentQvalue = getQvalue(state);
  labeledQvalue = (1 - RL.alpha) * currentQvalue + RL.alpha * (reward);
  backProp(currentQvalue, labeledQvalue);
}

int getState(int threshold)
{
  int state;

  if (SENSOR_3 < threshold - 10)
  {
    state = 0;
  }
  else if (SENSOR_3 < threshold - 5)
  {
    state = 1;
  }
  else if (SENSOR_3 < threshold)
  {
    state = 2;
  }
  else if (SENSOR_3 < threshold + 5)
  {
    state = 3;
  }
  else if (SENSOR_3 < threshold + 10)
  {
    state = 4;
  }
  else if (threshold + 10 < SENSOR_3)
  {
    state = 5;
  }
  else 
  {
    state = 6;
  };

  return state;
};

void doAction(int action)
{


  // actionToSpeedArray = [][];
  if (action == 0)
  {
    onRL(100, 100);
  }
  else if (action == 1)
  {
    onRL(80, 40);
  }
  else if (action == 2)
  {
    onRL(60, 30);
  }
  else if (action == 3)
  {
    onRL(80, 0);
  }
  else if (action == 4)
  {
    onRL(60, 0);
  }
  else if (action == 5)
  {
    onRL(40, 80);
  }
  else if (action == 6)
  {
    onRL(30, 60);
  }
  else if (action == 7)
  {
    onRL(0, 80);
  }
  else if (action == 8)
  {
    onRL(0, 60);
  }
  else if (action == 9)
  {
    onRL(100, 20);
  }
  else if (action == 10)
  {
    onRL(20, 100);
  }
};

double getReward(int state)
{
  double reward;
  reward = 0;

  if (SENSOR_3 < 40)
  {
    reward = 100;
  }
  else
  {
    reward = -50;
  }

  return reward;
};

///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////
// main
///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////
///////////////////////////////////////////////////////////////////////////////////////
struct NeuralNet NN;
struct RLParams RL;

task main()
{
  RL.alpha = 0.5;
  RL.gamma = 0.9;
  RL.epsilon = 5;
  RL.seed = 0;
  RL.threshold = 42;
  initNN();

  int numberOfAction = 5;
  int numberofState = 7;

  double reward = 0;
  int action = 0;
  int state = 0;
  int stateNext = 0;
  int i, j;

  state = getState(RL.threshold); // get initial state

  

  while (true)
  {
    action = epsilonGreedy(RL.epsilon, state, numberOfAction);
    doAction(action);

    ////////////
    Wait(10);
    ////////////

    stateNext = getState(RL.threshold); // get state from sensor
    reward = getReward(stateNext);

    // update weight
    updateWeight(state, stateNext, reward);

    state = stateNext;
  }
}
